"""
Performance Comparison and Benchmarking Tools
Compare Rust FFI vs Python implementations
"""

import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pandas as pd
from typing import Dict, List, Tuple, Any
import psutil
import os
import gc

# Import FFI module
try:
    import sys
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'python'))
    from ecg_processor_py import ECGAnalyzerFFI, ECGAnalysisConfig, FFI_AVAILABLE
except ImportError as e:
    print(f"‚ùå FFI Import Error: {e}")
    FFI_AVAILABLE = False


class PerformanceBenchmark:
    """Comprehensive performance benchmarking tool"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process()
        
    def generate_test_signals(self, lengths: List[int], count: int = 100) -> Dict[int, List[np.ndarray]]:
        """Generate test signals of various lengths"""
        signals = {}
        
        for length in lengths:
            signals[length] = []
            for _ in range(count):
                # Generate realistic ECG signal
                t = np.linspace(0, length / 360.0, length)
                hr = 60 + np.random.uniform(-10, 30)  # 50-90 BPM
                
                # ECG components
                signal = np.sin(2 * np.pi * hr / 60 * t)  # R peaks
                signal += 0.3 * np.sin(2 * np.pi * hr / 60 * t - 0.5)  # P waves
                signal += 0.2 * np.sin(2 * np.pi * hr / 60 * t + 0.5)  # T waves
                signal += 0.05 * np.random.randn(length)  # Noise
                
                signals[length].append(signal.astype(np.float64))
        
        return signals
    
    def benchmark_implementation(
        self, 
        analyzer, 
        signals: List[np.ndarray], 
        name: str
    ) -> Dict[str, Any]:
        """Benchmark a specific implementation"""
        print(f"üîç Benchmarking {name}...")
        
        results = {
            'name': name,
            'latencies': [],
            'memory_usage': [],
            'throughput': 0,
            'errors': 0,
            'signal_lengths': []
        }
        
        initial_memory = self.process.memory_info().rss
        
        for i, signal in enumerate(signals):
            try:
                # Memory before
                gc.collect()
                memory_before = self.process.memory_info().rss
                
                # Benchmark analysis
                start_time = time.time()
                result = analyzer.analyze(signal)
                end_time = time.time()
                
                # Memory after
                memory_after = self.process.memory_info().rss
                memory_used = memory_after - memory_before
                
                latency = (end_time - start_time) * 1000  # ms
                
                results['latencies'].append(latency)
                results['memory_usage'].append(memory_used)
                results['signal_lengths'].append(len(signal))
                
                if i % 10 == 0:
                    print(f"   Progress: {i+1}/{len(signals)} - Avg latency: {np.mean(results['latencies']):.2f}ms")
                
            except Exception as e:
                results['errors'] += 1
                print(f"   Error in signal {i}: {e}")
        
        # Calculate statistics
        if results['latencies']:
            results['avg_latency'] = np.mean(results['latencies'])
            results['p95_latency'] = np.percentile(results['latencies'], 95)
            results['p99_latency'] = np.percentile(results['latencies'], 99)
            results['min_latency'] = np.min(results['latencies'])
            results['max_latency'] = np.max(results['latencies'])
            results['std_latency'] = np.std(results['latencies'])
        
        if results['memory_usage']:
            results['avg_memory'] = np.mean(results['memory_usage'])
            results['max_memory'] = np.max(results['memory_usage'])
        
        total_time = sum(results['latencies']) / 1000  # Convert to seconds
        results['throughput'] = len(signals) / total_time if total_time > 0 else 0
        
        return results
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """Run comprehensive benchmark comparing FFI vs Python"""
        print("üöÄ Starting comprehensive performance benchmark...")
        
        # Test configurations
        signal_lengths = [1000, 5000, 10000, 20000]
        signals_per_length = 50
        
        # Generate test signals
        test_signals = self.generate_test_signals(signal_lengths, signals_per_length)
        
        # Initialize analyzers
        ffi_analyzer = ECGAnalyzerFFI(use_ffi=FFI_AVAILABLE)
        python_analyzer = ECGAnalyzerFFI(use_ffi=False)
        
        benchmark_results = {}
        
        for length in signal_lengths:
            print(f"\nüìä Testing signal length: {length}")
            signals = test_signals[length]
            
            # Benchmark Rust FFI
            if FFI_AVAILABLE:
                ffi_results = self.benchmark_implementation(
                    ffi_analyzer, signals, f"Rust FFI ({length})"
                )
                benchmark_results[f'ffi_{length}'] = ffi_results
            
            # Benchmark Python fallback
            python_results = self.benchmark_implementation(
                python_analyzer, signals, f"Python Fallback ({length})"
            )
            benchmark_results[f'python_{length}'] = python_results
        
        return benchmark_results
    
    def generate_comparison_report(self, results: Dict[str, Any]) -> str:
        """Generate detailed comparison report"""
        report = []
        report.append("# Python-Rust FFI Performance Comparison Report")
        report.append("=" * 60)
        report.append("")
        
        # Summary table
        report.append("## üìä Performance Summary")
        report.append("")
        report.append("| Signal Length | Implementation | Avg Latency (ms) | P95 Latency (ms) | Throughput (analyses/sec) | Memory (MB) |")
        report.append("|---------------|----------------|-------------------|-------------------|-------------------------|--------------|")
        
        for key, result in results.items():
            if 'avg_latency' in result:
                length = key.split('_')[1]
                impl = key.split('_')[0].upper()
                memory_mb = result.get('avg_memory', 0) / 1024 / 1024
                
                report.append(f"| {length:>13} | {impl:>14} | {result['avg_latency']:>17.2f} | "
                           f"{result.get('p95_latency', 0):>17.2f} | {result['throughput']:>23.1f} | "
                           f"{memory_mb:>10.2f} |")
        
        report.append("")
        
        # Performance improvements
        report.append("## üöÄ Performance Improvements")
        report.append("")
        
        for length in [1000, 5000, 10000, 20000]:
            ffi_key = f'ffi_{length}'
            python_key = f'python_{length}'
            
            if ffi_key in results and python_key in results:
                ffi_result = results[ffi_key]
                python_result = results[python_key]
                
                if ffi_result['avg_latency'] > 0 and python_result['avg_latency'] > 0:
                    speedup = python_result['avg_latency'] / ffi_result['avg_latency']
                    throughput_improvement = ffi_result['throughput'] / python_result['throughput']
                    
                    report.append(f"### Signal Length: {length}")
                    report.append(f"- **Latency Speedup**: {speedup:.1f}x faster")
                    report.append(f"- **Throughput Improvement**: {throughput_improvement:.1f}x higher")
                    report.append(f"- **Memory Efficiency**: "
                               f"{(python_result.get('avg_memory', 0) / ffi_result.get('avg_memory', 1)):.1f}x better")
                    report.append("")
        
        # Recommendations
        report.append("## üí° Recommendations")
        report.append("")
        
        avg_speedup = 0
        count = 0
        
        for length in [1000, 5000, 10000, 20000]:
            ffi_key = f'ffi_{length}'
            python_key = f'python_{length}'
            
            if ffi_key in results and python_key in results:
                ffi_result = results[ffi_key]
                python_result = results[python_key]
                
                if ffi_result['avg_latency'] > 0 and python_result['avg_latency'] > 0:
                    speedup = python_result['avg_latency'] / ffi_result['avg_latency']
                    avg_speedup += speedup
                    count += 1
        
        if count > 0:
            avg_speedup /= count
            
            if avg_speedup > 5:
                report.append("üéâ **EXCELLENT**: Rust FFI shows significant performance improvements")
            elif avg_speedup > 2:
                report.append("‚úÖ **GOOD**: Rust FFI provides meaningful performance benefits")
            else:
                report.append("‚ö†Ô∏è **MODERATE**: Performance improvements are modest")
            
            report.append(f"- Average speedup: {avg_speedup:.1f}x")
            report.append("")
        
        # Medical compliance
        report.append("## üè• Medical Compliance")
        report.append("")
        if FFI_AVAILABLE:
            report.append("‚úÖ **FERROCENE COMPLIANT**: Using certified medical compiler")
            report.append("‚úÖ **MEMORY SAFE**: Rust memory safety guarantees")
            report.append("‚úÖ **THREAD SAFE**: Concurrent access verified")
        else:
            report.append("‚ö†Ô∏è **FERROCENE UNAVAILABLE**: Using standard Rust toolchain")
            report.append("‚ö†Ô∏è **FALLBACK MODE**: Python implementation being used")
        
        return "\n".join(report)
    
    def save_results(self, results: Dict[str, Any], filename: str = "benchmark_results.json"):
        """Save benchmark results to file"""
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"üìÑ Results saved to {filename}")
    
    def generate_performance_plots(self, results: Dict[str, Any]):
        """Generate performance comparison plots"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            # Set up the plotting style
            plt.style.use('seaborn-v0_8')
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Python-Rust FFI Performance Comparison', fontsize=16, fontweight='bold')
            
            # Extract data for plotting
            signal_lengths = []
            ffi_latencies = []
            python_latencies = []
            ffi_throughput = []
            python_throughput = []
            
            for length in [1000, 5000, 10000, 20000]:
                ffi_key = f'ffi_{length}'
                python_key = f'python_{length}'
                
                signal_lengths.append(length)
                
                if ffi_key in results:
                    ffi_latencies.append(results[ffi_key].get('avg_latency', 0))
                    ffi_throughput.append(results[ffi_key].get('throughput', 0))
                else:
                    ffi_latencies.append(0)
                    ffi_throughput.append(0)
                
                if python_key in results:
                    python_latencies.append(results[python_key].get('avg_latency', 0))
                    python_throughput.append(results[python_key].get('throughput', 0))
                else:
                    python_latencies.append(0)
                    python_throughput.append(0)
            
            # Plot 1: Latency Comparison
            axes[0, 0].plot(signal_lengths, ffi_latencies, 'o-', label='Rust FFI', linewidth=2, markersize=8)
            axes[0, 0].plot(signal_lengths, python_latencies, 's-', label='Python Fallback', linewidth=2, markersize=8)
            axes[0, 0].set_xlabel('Signal Length (samples)')
            axes[0, 0].set_ylabel('Average Latency (ms)')
            axes[0, 0].set_title('Latency Comparison')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].set_yscale('log')
            
            # Plot 2: Throughput Comparison
            axes[0, 1].plot(signal_lengths, ffi_throughput, 'o-', label='Rust FFI', linewidth=2, markersize=8)
            axes[0, 1].plot(signal_lengths, python_throughput, 's-', label='Python Fallback', linewidth=2, markersize=8)
            axes[0, 1].set_xlabel('Signal Length (samples)')
            axes[0, 1].set_ylabel('Throughput (analyses/sec)')
            axes[0, 1].set_title('Throughput Comparison')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            # Plot 3: Speedup Factor
            speedups = [p/f if f > 0 else 0 for f, p in zip(ffi_latencies, python_latencies)]
            axes[1, 0].bar(signal_lengths, speedups, alpha=0.7, color='green')
            axes[1, 0].set_xlabel('Signal Length (samples)')
            axes[1, 0].set_ylabel('Speedup Factor (x)')
            axes[1, 0].set_title('Rust FFI Speedup')
            axes[1, 0].grid(True, alpha=0.3)
            
            # Plot 4: Performance Class
            performance_classes = []
            for latency in ffi_latencies:
                if latency < 5:
                    performance_classes.append('HIGH')
                elif latency < 10:
                    performance_classes.append('STANDARD')
                else:
                    performance_classes.append('SLOW')
            
            class_counts = {cls: performance_classes.count(cls) for cls in set(performance_classes)}
            axes[1, 1].pie(class_counts.values(), labels=class_counts.keys(), autopct='%1.1f%%')
            axes[1, 1].set_title('Performance Class Distribution')
            
            plt.tight_layout()
            plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            print("üìä Performance plots saved to 'performance_comparison.png'")
            
        except ImportError:
            print("‚ö†Ô∏è Matplotlib not available for plotting")
    
    def run_memory_profiling(self):
        """Run detailed memory profiling"""
        print("üîç Running memory profiling...")
        
        if not FFI_AVAILABLE:
            print("‚ö†Ô∏è FFI not available for memory profiling")
            return
        
        analyzer = ECGAnalyzerFFI(use_ffi=True)
        signal = np.random.randn(5000).astype(np.float64)
        
        memory_usage = []
        iterations = 1000
        
        for i in range(iterations):
            # Memory before
            gc.collect()
            memory_before = self.process.memory_info().rss
            
            # Analyze
            result = analyzer.analyze(signal)
            
            # Memory after
            memory_after = self.process.memory_info().rss
            memory_used = memory_after - memory_before
            memory_usage.append(memory_used)
            
            if i % 100 == 0:
                print(f"   Iteration {i}: {memory_used / 1024:.1f} KB")
        
        # Analyze memory usage
        memory_usage = np.array(memory_usage)
        
        print(f"\nüìä Memory Profiling Results:")
        print(f"   Average memory per analysis: {np.mean(memory_usage) / 1024:.1f} KB")
        print(f"   Max memory per analysis: {np.max(memory_usage) / 1024:.1f} KB")
        print(f"   Memory standard deviation: {np.std(memory_usage) / 1024:.1f} KB")
        print(f"   Total memory growth: {(memory_usage[-1] - memory_usage[0]) / 1024:.1f} KB")
        
        # Check for memory leaks
        if np.mean(memory_usage[-100:]) > np.mean(memory_usage[:100]) * 1.1:
            print("‚ö†Ô∏è Potential memory leak detected")
        else:
            print("‚úÖ No significant memory leaks detected")


def main():
    """Main benchmark runner"""
    print("üöÄ Python-Rust FFI Performance Benchmarking Tool")
    print("=" * 60)
    
    benchmark = PerformanceBenchmark()
    
    # Run comprehensive benchmark
    results = benchmark.run_comprehensive_benchmark()
    
    # Generate report
    report = benchmark.generate_comparison_report(results)
    print("\n" + report)
    
    # Save results
    benchmark.save_results(results)
    
    # Generate plots
    try:
        benchmark.generate_performance_plots(results)
    except Exception as e:
        print(f"‚ö†Ô∏è Plot generation failed: {e}")
    
    # Memory profiling
    benchmark.run_memory_profiling()
    
    print("\n‚úÖ Benchmark completed successfully!")


if __name__ == "__main__":
    main()
